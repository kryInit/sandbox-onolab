{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Full waveform inversion with Dask and Devito pickling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Here, we revisit [04_dask.ipynb: Full Waveform Inversion with Devito and Dask](https://github.com/devitocodes/devito/blob/master/examples/seismic/tutorials/04_dask.ipynb), but with a twist: we now want to show that it is possible to use pickle to serialize (deserialize) a Devito object structure into (from) a byte stream. This is specially useful in our example as the geometry of all source experiments remains essentially the same; only the source location changes. In other words, we can convert a `solver` object (built on top of generic Devito objects) into a byte stream to store it. Later on, this byte stream can then be retrieved and de-serialized back to an instance of the original `solver` object by the dask workers, and then be populated with the correct geometry for the i-th source location. We can still benefit from the simplicity of the example and create **only one `solver`** object which can be used to both generate the observed data set and to compute the predicted data and gradient in the FWI process. Further examples of pickling can be found [here](https://github.com/devitocodes/devito/blob/master/tests/test_pickle.py). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tutorial roughly follows the structure of [04_dask.ipynb](https://github.com/devitocodes/devito/blob/master/examples/seismic/tutorials/04_dask.ipynb). Technical details about [Dask](https://dask.pydata.org/en/latest/#dask) and [scipy.optimize.minimize](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html) will therefore treated only superficially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is different from 04_dask.ipynb\n",
    "\n",
    "* **The big difference between [04_dask.ipynb](https://github.com/devitocodes/devito/blob/master/examples/seismic/tutorials/04_dask.ipynb) and this tutorial is that in the former is created a `solver` object for each source in both forward modeling and FWI gradient kernels. While here only one `solver` object is created and reused along all the optimization process. This is done through pickling and unpickling respectively.**\n",
    "\n",
    "\n",
    "* Another difference between the tutorials is that the in [04_dask.ipynb](https://github.com/devitocodes/devito/blob/master/examples/seismic/tutorials/04_dask.ipynb) is created a list with the observed shots, and then each observed shot record of the list is passed as parameter to a single-shot FWI objective function executed in parallel using the `submit()` method. Here, a single observed shot record along information of its source location is stored in a dictionary, which is saved into a pickle file. Later, dask workers retrieve the corresponding pickled data when computing the gradient for a single shot. The same applies for the `model` object in the optimization process. It is serialized each time the model's velocity is updated. Then, dask workers unpickle data from file back to `model` object.\n",
    "\n",
    "\n",
    "* Moreover, there is a difference in the way that the global functional-gradient is obtained. In [04_dask.ipynb](https://github.com/devitocodes/devito/blob/master/examples/seismic/tutorials/04_dask.ipynb) we had to wait for all computations to finish via `wait(futures)` and then we sum the function values and gradients from all workers. Here, it is defined a type `fg_pair` so that a reduce function `sum` can be used, such function takes all the futures given to it and after they are completed, combine them to get the estimate of the global functional-gradient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## scipy.optimize.minimize \n",
    "\n",
    "As in [04_dask.ipynb](https://github.com/devitocodes/devito/blob/master/examples/seismic/tutorials/04_dask.ipynb), here we are going to focus on using L-BFGS via [scipy.optimize.minimize(method=’L-BFGS-B’)](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html#optimize-minimize-lbfgsb)\n",
    "\n",
    "```python\n",
    "scipy.optimize.minimize(fun, x0, args=(), method='L-BFGS-B', jac=None, bounds=None, tol=None, callback=None, options={'disp': None, 'maxls': 20, 'iprint': -1, 'gtol': 1e-05, 'eps': 1e-08, 'maxiter': 15000, 'ftol': 2.220446049250313e-09, 'maxcor': 10, 'maxfun': 15000})```\n",
    "\n",
    "The argument `fun` is a callable function that returns the misfit between the simulated and the observed data. If `jac` is a Boolean and is `True`, `fun` is assumed to return the gradient along with the objective function - as is our case when applying the adjoint-state method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask\n",
    "\n",
    "[Dask](https://dask.pydata.org/en/latest/#dask) is task-based parallelization framework for Python. It allows us to distribute our work among a collection of workers controlled by a central scheduler. Dask is [well-documented](https://docs.dask.org/en/latest/), flexible, an currently under active development.\n",
    "\n",
    "In the same way as in [04_dask.ipynb](https://github.com/devitocodes/devito/blob/master/examples/seismic/tutorials/04_dask.ipynb), we are going to use it here to parallelise the computation of the functional and gradient as this is the vast bulk of the computational expense of FWI and it is trivially parallel over data shots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward modeling\n",
    "\n",
    "We define the functions used for the forward modeling, as well as the other functions used in constructing and deconstructing Python/Devito objects to/from binary data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "#NBVAL_IGNORE_OUTPUT\n",
    "\n",
    "# Set up inversion parameters.\n",
    "param = {'t0': 0.,\n",
    "         'tn': 1000.,              # Simulation last 1 second (1000 ms)\n",
    "         'f0': 0.010,              # Source peak frequency is 10Hz (0.010 kHz)\n",
    "         'nshots': 5,              # Number of shots to create gradient from\n",
    "         'shape': (101, 101),      # Number of grid points (nx, nz).\n",
    "         'spacing': (10., 10.),    # Grid spacing in m. The domain size is now 1km by 1km.\n",
    "         'origin': (0, 0),         # Need origin to define relative source and receiver locations.\n",
    "         'nbl': 40}                # nbl thickness.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "from scipy import signal, optimize\n",
    "\n",
    "from devito import Grid\n",
    "\n",
    "from distributed import Client, LocalCluster, wait\n",
    "\n",
    "import cloudpickle as pickle\n",
    "\n",
    "# Import acoustic solver, source and receiver modules.\n",
    "from examples.seismic import Model, demo_model, AcquisitionGeometry, Receiver\n",
    "from examples.seismic.acoustic import AcousticWaveSolver\n",
    "from examples.seismic import AcquisitionGeometry\n",
    "\n",
    "# Import convenience function for plotting results\n",
    "from examples.seismic import plot_image\n",
    "from examples.seismic import plot_shotrecord\n",
    "\n",
    "\n",
    "def get_true_model():\n",
    "    ''' Define the test phantom; in this case we are using\n",
    "    a simple circle so we can easily see what is going on.\n",
    "    '''\n",
    "    return demo_model('circle-isotropic', vp_circle=3.0, vp_background=2.5, \n",
    "                      origin=param['origin'], shape=param['shape'],\n",
    "                      spacing=param['spacing'], nbl=param['nbl'])\n",
    "\n",
    "def get_initial_model():\n",
    "    '''The initial guess for the subsurface model.\n",
    "    '''\n",
    "    # Make sure both model are on the same grid\n",
    "    grid = get_true_model().grid\n",
    "    return demo_model('circle-isotropic', vp_circle=2.5, vp_background=2.5, \n",
    "                      origin=param['origin'], shape=param['shape'],\n",
    "                      spacing=param['spacing'], nbl=param['nbl'],\n",
    "                      grid=grid)\n",
    "\n",
    "def wrap_model(x, astype=None):\n",
    "    '''Wrap a flat array as a subsurface model.\n",
    "    '''\n",
    "    model = get_initial_model()\n",
    "    v_curr = 1.0/np.sqrt(x.reshape(model.shape))\n",
    "    \n",
    "    if astype:\n",
    "        model.update('vp', v_curr.astype(astype).reshape(model.shape))\n",
    "    else:\n",
    "        model.update('vp', v_curr.reshape(model.shape))\n",
    "    return model\n",
    "\n",
    "def load_model(filename):\n",
    "    \"\"\" Returns the current model. This is used by the\n",
    "    worker to get the current model.\n",
    "    \"\"\"\n",
    "    pkl = pickle.load(open(filename, \"rb\"))\n",
    "    \n",
    "    return pkl['model']\n",
    "\n",
    "def dump_model(filename, model):\n",
    "    ''' Dump model to disk.\n",
    "    '''\n",
    "    pickle.dump({'model':model}, open(filename, \"wb\"))\n",
    "    \n",
    "def load_shot_data(shot_id, dt):\n",
    "    ''' Load shot data from disk, resampling to the model time step.\n",
    "    '''\n",
    "    pkl = pickle.load(open(\"shot_%d.p\"%shot_id, \"rb\"))\n",
    "    \n",
    "    return pkl['geometry'], pkl['rec'].resample(dt)\n",
    "\n",
    "def dump_shot_data(shot_id, rec, geometry):\n",
    "    ''' Dump shot data to disk.\n",
    "    '''\n",
    "    pickle.dump({'rec':rec, 'geometry': geometry}, open('shot_%d.p'%shot_id, \"wb\"))\n",
    "    \n",
    "def generate_shotdata_i(param):\n",
    "    \"\"\" Inversion crime alert! Here the worker is creating the\n",
    "        'observed' data using the real model. For a real case\n",
    "        the worker would be reading seismic data from disk.\n",
    "    \"\"\"\n",
    "    # Reconstruct objects\n",
    "    with open(\"arguments.pkl\", \"rb\") as cp_file:\n",
    "        cp = pickle.load(cp_file)\n",
    "        \n",
    "    solver = cp['solver']\n",
    "\n",
    "    # source position changes according to the index\n",
    "    shot_id=param['shot_id']\n",
    "    \n",
    "    solver.geometry.src_positions[0,:]=[20, shot_id*1000./(param['nshots']-1)]\n",
    "    true_d = solver.forward()[0]\n",
    "    dump_shot_data(shot_id, true_d.resample(4.0), solver.geometry.src_positions)\n",
    "\n",
    "def generate_shotdata(solver):\n",
    "    # Pick devito objects (save on disk)\n",
    "    cp = {'solver': solver}\n",
    "    with open(\"arguments.pkl\", \"wb\") as cp_file:\n",
    "        pickle.dump(cp, cp_file) \n",
    "\n",
    "    work = [dict(param) for i in range(param['nshots'])]\n",
    "    # synthetic data is generated here twice: serial(loop below) and parallel (via dask map functionality) \n",
    "    for i in  range(param['nshots']):\n",
    "        work[i]['shot_id'] = i\n",
    "        generate_shotdata_i(work[i])\n",
    "\n",
    "    # Map worklist to cluster, We pass our function and the dictionary to the map() function of the client\n",
    "    # This returns a list of futures that represents each task\n",
    "    futures = c.map(generate_shotdata_i, work)\n",
    "\n",
    "    # Wait for all futures\n",
    "    wait(futures)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "#NBVAL_IGNORE_OUTPUT\n",
    "from examples.seismic import plot_shotrecord\n",
    "\n",
    "# Client setup\n",
    "cluster = LocalCluster(n_workers=2, death_timeout=600)\n",
    "c = Client(cluster)\n",
    "\n",
    "# Generate shot data.\n",
    "true_model = get_true_model()\n",
    "# Source coords definition\n",
    "src_coordinates = np.empty((1, len(param['shape'])))\n",
    "# Number of receiver locations per shot.\n",
    "nreceivers = 101\n",
    "# Set up receiver data and geometry.\n",
    "rec_coordinates = np.empty((nreceivers, len(param['shape'])))\n",
    "rec_coordinates[:, 1] = np.linspace(param['spacing'][0], true_model.domain_size[0] - param['spacing'][0], num=nreceivers)\n",
    "rec_coordinates[:, 0] = 980. # 20m from the right end\n",
    "# Geometry \n",
    "geometry = AcquisitionGeometry(true_model, rec_coordinates, src_coordinates,\n",
    "                               param['t0'], param['tn'], src_type='Ricker',\n",
    "                               f0=param['f0'])\n",
    "# Set up solver\n",
    "solver = AcousticWaveSolver(true_model, geometry, space_order=4)\n",
    "generate_shotdata(solver)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask specifics\n",
    "\n",
    "Previously in [03_fwi.ipynb](https://github.com/devitocodes/devito/blob/master/examples/seismic/tutorials/03_fwi.ipynb), we defined a function to calculate the individual contribution to the functional and gradient for each shot, which was then used in a loop over all shots. However, when using distributed frameworks such as Dask we instead think in terms of creating a worklist which gets *mapped* onto the worker pool. The sum reduction is also performed in parallel. For now however we assume that the scipy.optimize.minimize itself is running on the *master* process; this is a reasonable simplification because the computational cost of calculating (f, g) far exceeds the other compute costs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we want to be able to use standard reduction operators such as sum on (f, g) we first define it as a type so that we can define the `__add__` (and `__radd__` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "# Define a type to store the functional and gradient.\n",
    "class fg_pair:\n",
    "    def __init__(self, f, g):\n",
    "        self.f = f\n",
    "        self.g = g\n",
    "    \n",
    "    def __add__(self, other):\n",
    "        f = self.f + other.f\n",
    "        g = self.g + other.g\n",
    "        \n",
    "        return fg_pair(f, g)\n",
    "    \n",
    "    def __radd__(self, other):\n",
    "        if other == 0:\n",
    "            return self\n",
    "        else:\n",
    "            return self.__add__(other)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create operators for gradient based inversion\n",
    "To perform the inversion we are going to use [scipy.optimize.minimize(method=’L-BFGS-B’)](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html#optimize-minimize-lbfgsb).\n",
    "\n",
    "First we define the functional, ```f```, and gradient, ```g```, operator (i.e. the function ```fun```) for a single shot of data. This is the work that is going to be performed by the worker on a unit of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "#NBVAL_IGNORE_OUTPUT\n",
    "from devito import Function\n",
    "\n",
    "# Create FWI gradient kernel for a single shot\n",
    "def fwi_gradient_i(param):\n",
    "\n",
    "    # Load the current model and the shot data for this worker.\n",
    "    # Note, unlike the serial example the model is not passed in\n",
    "    # as an argument. Broadcasting large datasets is considered\n",
    "    # a programming anti-pattern and at the time of writing it\n",
    "    # it only worked reliably with Dask master. Therefore, the\n",
    "    # the model is communicated via a file.\n",
    "    model0 = load_model(param['model'])\n",
    "    \n",
    "    dt = model0.critical_dt\n",
    "    nbl = model0.nbl\n",
    "\n",
    "    # Get src_position and data\n",
    "    src_positions, rec = load_shot_data(param['shot_id'], dt)\n",
    "\n",
    "    # Set up solver -- load the solver used above in the generation of the syntethic data.    \n",
    "    with open(\"arguments.pkl\", \"rb\") as cp_file:\n",
    "        cp = pickle.load(cp_file)\n",
    "    solver = cp['solver']\n",
    "    \n",
    "    # Set attributes to solver\n",
    "    solver.geometry.src_positions=src_positions\n",
    "    solver.geometry.resample(dt)\n",
    "\n",
    "    # Compute simulated data and full forward wavefield u0\n",
    "    d, u0 = solver.forward(vp=model0.vp, dt=dt, save=True)[0:2]\n",
    "        \n",
    "    # Compute the data misfit (residual) and objective function\n",
    "    residual = Receiver(name='rec', grid=model0.grid,\n",
    "                        time_range=solver.geometry.time_axis,\n",
    "                        coordinates=solver.geometry.rec_positions)\n",
    "\n",
    "    #residual.data[:] = d.data[:residual.shape[0], :] - rec.data[:residual.shape[0], :]\n",
    "    residual.data[:] = d.data[:] - rec.data[0:d.data.shape[0], :]\n",
    "    f = .5*np.linalg.norm(residual.data.flatten())**2\n",
    "\n",
    "    # Compute gradient using the adjoint-state method. Note, this\n",
    "    # backpropagates the data misfit through the model.\n",
    "    grad = Function(name=\"grad\", grid=model0.grid)\n",
    "    solver.gradient(rec=residual, u=u0, vp=model0.vp, dt=dt, grad=grad)\n",
    "    \n",
    "    # Copying here to avoid a (probably overzealous) destructor deleting\n",
    "    # the gradient before Dask has had a chance to communicate it.\n",
    "    g = np.array(grad.data[:])[nbl:-nbl, nbl:-nbl]    \n",
    "    \n",
    "    # return the objective functional and gradient.\n",
    "    return fg_pair(f, g)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the global functional-gradient operator. This does the following:\n",
    "* Maps the worklist (shots) to the workers so that the invidual contributions to (f, g) are computed.\n",
    "* Sum individual contributions to (f, g) and returns the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "def fwi_gradient(model, param):\n",
    "    # Dump a copy of the current model for the workers\n",
    "    # to pick up when they are ready.\n",
    "    param['model'] = \"model_0.p\"\n",
    "    dump_model(param['model'], wrap_model(model))\n",
    "\n",
    "    # Define work list\n",
    "    work = [dict(param) for i in range(param['nshots'])]\n",
    "    for i in  range(param['nshots']):\n",
    "        work[i]['shot_id'] = i\n",
    "        \n",
    "    # Distribute worklist to workers.\n",
    "    fgi = c.map(fwi_gradient_i, work, retries=1)\n",
    "    \n",
    "    # Perform reduction.\n",
    "    fg = c.submit(sum, fgi).result()\n",
    "    \n",
    "    # L-BFGS in scipy expects a flat array in 64-bit floats.\n",
    "    return fg.f, fg.g.flatten().astype(np.float64)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FWI with L-BFGS-B\n",
    "Equipped with a function to calculate the functional and gradient, we are finally ready to define the optimization function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "from scipy import optimize\n",
    "\n",
    "# Many optimization methods in scipy.optimize.minimize accept a callback\n",
    "# function that can operate on the solution after every iteration. Here\n",
    "# we use this to monitor the true relative solution error.\n",
    "relative_error = []\n",
    "def fwi_callbacks(x):    \n",
    "    # Calculate true relative error\n",
    "    true_vp = get_true_model().vp.data[param['nbl']:-param['nbl'], param['nbl']:-param['nbl']]\n",
    "    true_m = 1.0 / (true_vp.reshape(-1).astype(np.float64))**2\n",
    "    relative_error.append(np.linalg.norm((x-true_m)/true_m))\n",
    "\n",
    "# FWI with L-BFGS\n",
    "ftol = 0.1\n",
    "maxiter = 5\n",
    "\n",
    "def fwi(model, param, ftol=ftol, maxiter=maxiter):\n",
    "    # Initial guess\n",
    "    v0 = model.vp.data[param['nbl']:-param['nbl'], param['nbl']:-param['nbl']]\n",
    "    m0 = 1.0 / (v0.reshape(-1).astype(np.float64))**2\n",
    "    \n",
    "    # Define bounding box constraints on the solution.\n",
    "    vmin = 1.4    # do not allow velocities slower than water\n",
    "    vmax = 4.0\n",
    "    bounds = [(1.0/vmax**2, 1.0/vmin**2) for _ in range(np.prod(model.shape))]    # in [s^2/km^2]\n",
    "    \n",
    "    result = optimize.minimize(fwi_gradient,\n",
    "                               m0, args=(param, ), method='L-BFGS-B', jac=True,\n",
    "                               bounds=bounds, callback=fwi_callbacks,\n",
    "                               options={'ftol':ftol,\n",
    "                                        'maxiter':maxiter,\n",
    "                                        'disp':True})\n",
    "\n",
    "    return result"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply our FWI function and have a look at the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "#NBVAL_IGNORE_OUTPUT\n",
    "\n",
    "model0 = get_initial_model()\n",
    "\n",
    "# Baby steps\n",
    "result = fwi(model0, param)\n",
    "\n",
    "# Print out results of optimizer.\n",
    "print(result)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "#NBVAL_SKIP\n",
    "\n",
    "# Plot FWI result\n",
    "from examples.seismic import plot_image\n",
    "\n",
    "slices = tuple(slice(param['nbl'],-param['nbl']) for _ in range(2))\n",
    "vp = 1.0/np.sqrt(result['x'].reshape(true_model.shape))\n",
    "plot_image(true_model.vp.data[slices], vmin=2.4, vmax=2.8, cmap=\"cividis\")\n",
    "plot_image(vp, vmin=2.4, vmax=2.8, cmap=\"cividis\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "#NBVAL_SKIP\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot model error\n",
    "plt.plot(range(1, maxiter+1), relative_error); plt.xlabel('Iteration number'); plt.ylabel('L2-model error')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed in last figures, the results we obtain are exactly the same to the ones obtained in [04_dask.ipynb](https://github.com/devitocodes/devito/blob/master/examples/seismic/tutorials/04_dask.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
